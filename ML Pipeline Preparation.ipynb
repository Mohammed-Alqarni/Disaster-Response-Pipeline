{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# libraries for data processing and machine learning\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np  # For numerical operations\n",
    "import os  # For operating system interactions\n",
    "import pickle  # For object serialization\n",
    "from sqlalchemy import create_engine  # For database interactions\n",
    "import re  # For regular expressions\n",
    "import nltk  # For natural language processing\n",
    "from sklearn.base import BaseEstimator, TransformerMixin  # For custom transformers\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.multioutput import MultiOutputClassifier  # For multi-output classification\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier  # For ensemble methods\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer  # For text feature extraction\n",
    "from nltk.tokenize import word_tokenize  # For tokenization\n",
    "from nltk.stem import WordNetLemmatizer  # For lemmatization\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion  # For building pipelines\n",
    "from sklearn.model_selection import GridSearchCV  # For hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, fbeta_score, classification_report  # For model evaluation\n",
    "from scipy.stats import hmean  # For harmonic mean\n",
    "from scipy.stats.mstats import gmean  # For geometric mean\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing tables in DisasterResponse.db: [('Message',)]\n",
      "Data loaded successfully from DisasterResponse.db.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define the SQLite engine for a potentially different database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "\n",
    "# List all tables in the new database\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = result.fetchall()\n",
    "    print(\"Existing tables in DisasterResponse.db:\", tables)\n",
    "\n",
    "# Use the correct table name found in the database\n",
    "if tables:\n",
    "    table_name = tables[0][0]  # Just using the first table found\n",
    "    df = pd.read_sql_table(table_name, engine)\n",
    "    print(\"Data loaded successfully from DisasterResponse.db.\")\n",
    "else:\n",
    "    print(\"No tables found in DisasterResponse.db.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def tokenize(text):\n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Convert tokens to lowercase and remove punctuation\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def tokenize(text):\n",
    "    # Regular expression for detecting URLs\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    # Replace URLs with a placeholder\n",
    "    text = url_pattern.sub('urlplaceholder', text)\n",
    "    \n",
    "    # Tokenize text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Process tokens: lemmatize, convert to lowercase, and strip whitespace\n",
    "    processed_tokens = [lemmatizer.lemmatize(token).lower().strip() for token in tokens]\n",
    "    \n",
    "    return processed_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def build_pipeline():\n",
    "    vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "    transformer = TfidfTransformer()\n",
    "    classifier = MultiOutputClassifier(RandomForestClassifier())\n",
    "    \n",
    "    # Assemble the pipeline\n",
    "    text_pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('transformer', transformer),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    return text_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class WordCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Count the number of words in each document\n",
    "        return [[len(doc.split())] for doc in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def create_pipeline():\n",
    "    # Define text processing pipeline\n",
    "    text_processing = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf_transformer', TfidfTransformer())\n",
    "    ])\n",
    "    \n",
    "    # Define feature union\n",
    "    feature_union = FeatureUnion([\n",
    "        ('text', text_processing),\n",
    "        ('verb_extractor', StartingVerbExtractor())\n",
    "    ])\n",
    "    \n",
    "    # Define full pipeline\n",
    "    full_pipeline = Pipeline([\n",
    "        ('features', feature_union),\n",
    "        ('classifier', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "    ])\n",
    "    \n",
    "    return full_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Load data from the CSV file\n",
    "df_from_csv = pd.read_csv('categories.csv')  # Update this path if necessary\n",
    "\n",
    "# Connect to the database\n",
    "engine = create_engine('sqlite:///ETL_Preparation.db')\n",
    "\n",
    "# Create a new table in the database\n",
    "df_from_csv.to_sql('ETL_Preparation', engine, index=False, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables: [('ETL_Preparation',)]\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connect to the database\n",
    "engine = create_engine('sqlite:///ETL_Preparation.db')\n",
    "\n",
    "# List all tables\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = result.fetchall()\n",
    "    print(\"Tables:\", tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14164    16810\n",
      "6797      7700\n",
      "2115      2428\n",
      "16960    19902\n",
      "17964    21020\n",
      "Name: id, dtype: object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())  # Display the first few entries\n",
    "print(X_train.dtype)   # Check the data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(str)\n",
    "X_test = X_test.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                            message  \\\n",
      "0   2  Weather update - a cold front from Cuba that c...   \n",
      "1   7            Is the Hurricane over or is it not over   \n",
      "2   8                    Looking for someone but no name   \n",
      "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
      "4  12  says: west side of Haiti, rest of the country ...   \n",
      "\n",
      "                                            original   genre  \n",
      "0  Un front froid se retrouve sur Cuba ce matin. ...  direct  \n",
      "1                 Cyclone nan fini osinon li pa fini  direct  \n",
      "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct  \n",
      "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct  \n",
      "4  facade ouest d Haiti et le reste du pays aujou...  direct  \n",
      "Index(['id', 'message', 'original', 'genre'], dtype='object')\n",
      "Pipeline(memory=None,\n",
      "     steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('text_pipeline', Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_d...stimators=50, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False))])\n",
      "Accuracy: 0.935047619048\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'messages.csv'\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and column names to inspect the data\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "\n",
    "# Update these lines based on your actual column names\n",
    "X = df['message']  # Column with text data\n",
    "y = df['genre']    # Update 'genre' with your actual target column name\n",
    "\n",
    "# Ensure X is of type str\n",
    "X = X.astype(str)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('features', FeatureUnion(transformer_list=[\n",
    "        ('text_pipeline', Pipeline(steps=[\n",
    "            ('vect', CountVectorizer(analyzer='word', lowercase=True)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier(n_estimators=50, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Print the pipeline steps to show the structure\n",
    "print(pipeline)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['direct'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('messages.csv')\n",
    "\n",
    "# Prepare features and target\n",
    "X = df['message'].astype(str)\n",
    "y = pd.get_dummies(df['genre'])  # Convert target to one-hot encoding if needed\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(lowercase=True)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier(n_estimators=50, random_state=42))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict new message\n",
    "msg = ['Hello I see fire in the street and many houses are destroyed, homeless people everywhere']\n",
    "test_output = pipeline.predict(msg)\n",
    "\n",
    "# Print predicted labels\n",
    "label_columns = y.columns  # Column names of y_train\n",
    "predicted_labels = label_columns[test_output.flatten() == 1]\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('messages.csv')\n",
    "\n",
    "# Prepare features and target\n",
    "X = df['message'].astype(str)\n",
    "y = df['genre']  # Single-label target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(lowercase=True)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier(n_estimators=50, random_state=42))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict new message\n",
    "msg = ['Hello I see fire in the street and many houses are destroyed, homeless people everywhere']\n",
    "test_output = pipeline.predict(msg)\n",
    "\n",
    "# Print predicted label\n",
    "print(test_output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['direct']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('messages.csv')\n",
    "\n",
    "# Prepare features and target\n",
    "X = df['message'].astype(str)\n",
    "y = pd.get_dummies(df['genre'])  # Ensure this is a DataFrame with binary columns for each label\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(lowercase=True)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier(n_estimators=50, random_state=42))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict new message\n",
    "msg = ['Hello I see fire in the street and many houses are destroyed, homeless people everywhere']\n",
    "test_output = pipeline.predict(msg)\n",
    "\n",
    "# Print predicted labels\n",
    "label_columns = y.columns  # Column names of y_train\n",
    "predicted_labels = label_columns[test_output.flatten() == 1]\n",
    "print(predicted_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct :\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     direct       0.97      0.94      0.95      3075\n",
      "\n",
      "avg / total       0.95      0.95      0.95      5250\n",
      "\n",
      "...................................................\n",
      "news :\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       news       0.94      0.97      0.96      2642\n",
      "\n",
      "avg / total       0.96      0.96      0.96      5250\n",
      "\n",
      "...................................................\n",
      "social :\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     social       0.96      1.00      0.98      4783\n",
      "\n",
      "avg / total       0.97      0.97      0.96      5250\n",
      "\n",
      "...................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 2, does not match size of target_names, 1\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('messages.csv')\n",
    "\n",
    "# Prepare features and target\n",
    "X = df['message'].astype(str)\n",
    "y = pd.get_dummies(df['genre'])  # Ensure y is a DataFrame with binary columns for each label\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(lowercase=True)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier(n_estimators=50, random_state=42))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Get category names\n",
    "category_names = y.columns\n",
    "\n",
    "# Print classification reports for each category\n",
    "for i in range(len(category_names)):\n",
    "    print(f\"{category_names[i]} :\")\n",
    "    print(classification_report(y_test.iloc[:, i], y_pred[:, i], target_names=[category_names[i]]))\n",
    "    print('...................................................')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score\n",
    "from scipy.stats import gmean\n",
    "\n",
    "def multioutput_fscore(y_true, y_pred, beta=1):\n",
    "    '''\n",
    "    Calculates the geometric mean of the F-beta scores for all \n",
    "    predicted classes. Assumes y_true and y_pred are either \n",
    "    numpy arrays or pandas DataFrames.\n",
    "    '''\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "\n",
    "    scores = [fbeta_score(y_true[:, i], y_pred[:, i], beta, average='weighted') \n",
    "              for i in range(y_true.shape[1])]\n",
    "\n",
    "    # Convert to numpy array and filter out perfect scores\n",
    "    scores = np.array(scores)\n",
    "    scores = scores[scores < 1]\n",
    "\n",
    "    # Return the geometric mean of the F-beta scores\n",
    "    return gmean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average overall accuracy: 95.62%\n",
      "F1 score (custom definition): 95.50%\n"
     ]
    }
   ],
   "source": [
    "# Calculate multi-output F1 score with custom definition\n",
    "multi_f1 = multioutput_fscore(y_test, y_pred, beta=1)\n",
    "\n",
    "# Calculate overall accuracy as the mean of equality between predicted and true labels\n",
    "overall_accuracy = (y_pred == y_test).mean().mean()\n",
    "\n",
    "# Print results with formatted output\n",
    "print(f'Average overall accuracy: {overall_accuracy:.2%}')\n",
    "print(f'F1 score (custom definition): {multi_f1:.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'message', 'original', 'genre'], dtype='object')\n",
      "   id                                            message  \\\n",
      "0   2  Weather update - a cold front from Cuba that c...   \n",
      "1   7            Is the Hurricane over or is it not over   \n",
      "2   8                    Looking for someone but no name   \n",
      "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
      "4  12  says: west side of Haiti, rest of the country ...   \n",
      "\n",
      "                                            original   genre  \n",
      "0  Un front froid se retrouve sur Cuba ce matin. ...  direct  \n",
      "1                 Cyclone nan fini osinon li pa fini  direct  \n",
      "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct  \n",
      "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct  \n",
      "4  facade ouest d Haiti et le reste du pays aujou...  direct  \n"
     ]
    }
   ],
   "source": [
    "# Print the column names and first few rows of the DataFrame\n",
    "print(data.columns)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'message', 'original', 'genre'], dtype='object')\n",
      "   id                                            message  \\\n",
      "0   2  Weather update - a cold front from Cuba that c...   \n",
      "1   7            Is the Hurricane over or is it not over   \n",
      "2   8                    Looking for someone but no name   \n",
      "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
      "4  12  says: west side of Haiti, rest of the country ...   \n",
      "\n",
      "                                            original   genre  \n",
      "0  Un front froid se retrouve sur Cuba ce matin. ...  direct  \n",
      "1                 Cyclone nan fini osinon li pa fini  direct  \n",
      "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct  \n",
      "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct  \n",
      "4  facade ouest d Haiti et le reste du pays aujou...  direct  \n",
      "Accuracy: 0.92\n",
      "Precision: 0.92\n",
      "Recall: 0.92\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     direct       0.95      0.89      0.91      3236\n",
      "       news       0.91      0.97      0.94      3924\n",
      "     social       0.79      0.76      0.77       715\n",
      "\n",
      "avg / total       0.92      0.92      0.92      7875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('messages.csv')\n",
    "\n",
    "# Display the first few rows and column names\n",
    "print(data.columns)\n",
    "print(data.head())\n",
    "\n",
    "# Define features and target\n",
    "X = data['message']  # Features\n",
    "y = data['genre']    # Target\n",
    "\n",
    "# Convert text data to features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features if needed\n",
    "scaler = StandardScaler(with_mean=False)  # With mean=False for sparse matrices\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "Precision: 0.93\n",
      "Recall: 0.92\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     direct       0.94      0.92      0.93      3236\n",
      "       news       0.90      0.99      0.94      3924\n",
      "     social       0.98      0.59      0.74       715\n",
      "\n",
      "avg / total       0.93      0.92      0.92      7875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('messages.csv')\n",
    "\n",
    "# Define features and target\n",
    "X = data['message']  # Features\n",
    "y = data['genre']    # Target\n",
    "\n",
    "# Convert text data to features with bigrams and trigrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))  # Unigrams, bigrams, and trigrams\n",
    "X_features = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the filename for saving the model\n",
    "model_filename = 'classifier_model.pkl'\n",
    "\n",
    "# Save the trained model to a file\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# To load the model from the file\n",
    "# with open(model_filename, 'rb') as file:\n",
    "#     loaded_model = pickle.load(file)\n",
    "#     accuracy = loaded_model.score(X_train, y_train)\n",
    "#     print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
